"""
03_feature_engineering.py
==========================
ç¬¬ä¸‰æ­¥ï¼šç‰¹å¾µå·¥ç¨‹

åŠŸèƒ½ï¼š
1. å»ºç«‹å ´æ™¯å±¤ç´šç‰¹å¾µï¼ˆå®ˆæ³•ã€å¤šæ•¸ã€è¡çªç­‰ï¼‰
2. å»ºç«‹ä½¿ç”¨è€…é“å¾·å´å¯«
3. åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†
4. ç”¢ç”Ÿç‰¹å¾µèªªæ˜æ–‡ä»¶

åŸ·è¡Œæ–¹å¼ï¼š
    python scripts/03_feature_engineering.py
"""

import sys
from pathlib import Path

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ å…¥è·¯å¾‘
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

from src.data.feature_engineer import FeatureEngineer
import pandas as pd
import logging
from datetime import datetime
import json

def setup_file_logger(log_dir: str = 'outputs/logs') -> logging.Logger:
    """è¨­å®šæª”æ¡ˆæ—¥èªŒè¨˜éŒ„å™¨"""
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    
    log_file = log_path / 'feature_engineering.log'
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    file_handler.setFormatter(formatter)
    
    logger = logging.getLogger()
    logger.addHandler(file_handler)
    logger.setLevel(logging.INFO)
    
    return logger

def save_featured_data(df: pd.DataFrame, output_dir: str = 'data/processed'):
    """å„²å­˜å¢åŠ ç‰¹å¾µçš„è³‡æ–™"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'featured_data.csv'
    
    print(f"\nå„²å­˜ç‰¹å¾µåŒ–è³‡æ–™...")
    df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    file_size_mb = output_file.stat().st_size / 1024**2
    print(f"âœ… å·²å„²å­˜: {output_file}")
    print(f"   æª”æ¡ˆå¤§å°: {file_size_mb:.2f} MB")
    print(f"   æ¬„ä½æ•¸: {len(df.columns)}")

def save_user_profiles(profiles_df: pd.DataFrame, output_dir: str = 'data/processed'):
    """å„²å­˜ä½¿ç”¨è€…é“å¾·å´å¯«"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    output_file = output_path / 'user_moral_profiles.csv'
    
    print(f"\nå„²å­˜ä½¿ç”¨è€…é“å¾·å´å¯«...")
    profiles_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    print(f"âœ… å·²å„²å­˜: {output_file}")
    print(f"   ä½¿ç”¨è€…æ•¸: {len(profiles_df):,}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰åˆ†å‰²æ¨™è¨˜
    if 'split' in profiles_df.columns:
        train_count = (profiles_df['split'] == 'train').sum()
        test_count = (profiles_df['split'] == 'test').sum()
        print(f"   è¨“ç·´é›†: {train_count:,} ä½")
        print(f"   æ¸¬è©¦é›†: {test_count:,} ä½")
        print(f"   âš ï¸  æ³¨æ„ï¼šå´å¯«å·²åˆ†åˆ¥åŸºæ–¼è¨“ç·´/æ¸¬è©¦é›†è¨ˆç®—ï¼Œé¿å…è³‡æ–™æ´©æ¼")

def save_train_test_split(train_df: pd.DataFrame, 
                          test_df: pd.DataFrame,
                          output_dir: str = 'data/processed'):
    """å„²å­˜è¨“ç·´/æ¸¬è©¦é›†"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nå„²å­˜è¨“ç·´/æ¸¬è©¦é›†...")
    
    # å„²å­˜è¨“ç·´é›†
    train_file = output_path / 'train_data.csv'
    train_df.to_csv(train_file, index=False, encoding='utf-8-sig')
    train_size_mb = train_file.stat().st_size / 1024**2
    print(f"âœ… è¨“ç·´é›†: {train_file}")
    print(f"   {len(train_df):,} è¡Œ, {train_size_mb:.2f} MB")
    
    # å„²å­˜æ¸¬è©¦é›†
    test_file = output_path / 'test_data.csv'
    test_df.to_csv(test_file, index=False, encoding='utf-8-sig')
    test_size_mb = test_file.stat().st_size / 1024**2
    print(f"âœ… æ¸¬è©¦é›†: {test_file}")
    print(f"   {len(test_df):,} è¡Œ, {test_size_mb:.2f} MB")
    
    # å„²å­˜åˆ†å‰²ç´¢å¼•
    split_index = {
        'train_users': train_df['UserID'].unique().tolist(),
        'test_users': test_df['UserID'].unique().tolist(),
        'split_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'test_size': len(test_df) / (len(train_df) + len(test_df))
    }
    
    index_file = output_path / 'train_test_split.json'
    with open(index_file, 'w', encoding='utf-8') as f:
        json.dump(split_index, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… åˆ†å‰²ç´¢å¼•: {index_file}")

def save_feature_descriptions(descriptions: dict, output_dir: str = 'outputs/tables/chapter2'):
    """å„²å­˜ç‰¹å¾µèªªæ˜æ–‡ä»¶"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nç”Ÿæˆç‰¹å¾µèªªæ˜æ–‡ä»¶...")
    
    # 1. CSVæ ¼å¼
    desc_df = pd.DataFrame([
        {'ç‰¹å¾µåç¨±': name, 'èªªæ˜': desc}
        for name, desc in descriptions.items()
    ])
    
    csv_file = output_path / 'feature_descriptions.csv'
    desc_df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    print(f"âœ… CSVæ ¼å¼: {csv_file}")
    
    # 2. JSONæ ¼å¼
    json_file = output_path / 'feature_descriptions.json'
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(descriptions, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSONæ ¼å¼: {json_file}")

def generate_feature_statistics(df: pd.DataFrame, output_dir: str = 'outputs/tables/chapter2'):
    """ç”Ÿæˆç‰¹å¾µçµ±è¨ˆå ±å‘Š"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nç”Ÿæˆç‰¹å¾µçµ±è¨ˆå ±å‘Š...")
    
    # å ´æ™¯ç‰¹å¾µçµ±è¨ˆ
    scenario_stats = []
    
    feature_cols = ['is_lawful', 'is_majority', 'chose_lawful', 
                   'chose_majority', 'lawful_vs_majority_conflict']
    
    for col in feature_cols:
        if col in df.columns:
            scenario_stats.append({
                'ç‰¹å¾µ': col,
                'å¹³å‡å€¼': f"{df[col].mean():.3f}",
                'æ¨™æº–å·®': f"{df[col].std():.3f}",
                'æœ€å°å€¼': int(df[col].min()),
                'æœ€å¤§å€¼': int(df[col].max()),
                'ç¸½å’Œ': f"{df[col].sum():,}"
            })
    
    stats_df = pd.DataFrame(scenario_stats)
    stats_file = output_path / 'scenario_feature_stats.csv'
    stats_df.to_csv(stats_file, index=False, encoding='utf-8-sig')
    print(f"âœ… å ´æ™¯ç‰¹å¾µçµ±è¨ˆ: {stats_file}")

def generate_markdown_report(df: pd.DataFrame,
                            profiles_df: pd.DataFrame,
                            descriptions: dict,
                            output_dir: str = 'report/drafts'):
    """ç”ŸæˆMarkdownæ ¼å¼çš„å ±å‘Šè‰ç¨¿"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    report_file = output_path / 'chapter2_section3_feature_engineering.md'
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# ç¬¬2ç«  è³‡æ–™è™•ç†\n\n")
        f.write("## 2.3 ç‰¹å¾µå·¥ç¨‹\n\n")
        
        f.write("### ç‰¹å¾µå·¥ç¨‹ç›®æ¨™\n\n")
        f.write("ç‚ºäº†æ·±å…¥åˆ†æã€Œå®ˆæ³•vs.æ•ˆç›Šã€çš„é“å¾·å…©é›£ï¼Œæœ¬ç ”ç©¶å»ºç«‹ä»¥ä¸‹ç‰¹å¾µï¼š\n\n")
        f.write("1. **å ´æ™¯å±¤ç´šç‰¹å¾µ**ï¼šæ¨™è¨˜æ¯å€‹é¸æ“‡çš„å®ˆæ³•æ€§ã€å¤šæ•¸æ€§ã€è¡çªæ€§\n")
        f.write("2. **åœ‹å®¶å±¤ç´šç‰¹å¾µ**ï¼šæ•´åˆåœ‹å®¶çš„é“å¾·åå¥½ AMCE å€¼ï¼ˆç”¨æ–¼éšå±¤æ¨¡å‹ï¼‰\n")
        f.write("3. **ä½¿ç”¨è€…é“å¾·å´å¯«**ï¼šé‡åŒ–æ¯ä½ä½¿ç”¨è€…çš„é“å¾·å‚¾å‘ï¼ˆåƒ…ç”¨æ–¼æ¢ç´¢ï¼‰\n\n")
        
        f.write("### å ´æ™¯å±¤ç´šç‰¹å¾µ\n\n")
        f.write("| ç‰¹å¾µåç¨± | èªªæ˜ | å¹³å‡å€¼ |\n")
        f.write("|---------|------|--------|\n")
        
        scenario_features = ['is_lawful', 'is_majority', 'chose_lawful', 
                            'chose_majority', 'lawful_vs_majority_conflict']
        
        for feat in scenario_features:
            if feat in df.columns:
                desc = descriptions.get(feat, '')
                mean_val = df[feat].mean()
                f.write(f"| {feat} | {desc} | {mean_val:.3f} |\n")
        
        f.write("\n#### é—œéµç™¼ç¾\n\n")
        
        # çµ±è¨ˆå®ˆæ³•é¸æ“‡ç‡
        if 'chose_lawful' in df.columns and 'Saved' in df.columns:
            chose_lawful_rate = df[df['Saved'] == 1]['is_lawful'].mean()
            f.write(f"- **å®ˆæ³•é¸æ“‡ç‡**: {chose_lawful_rate*100:.1f}%\n")
        
        # çµ±è¨ˆå¤šæ•¸é¸æ“‡ç‡
        if 'chose_majority' in df.columns and 'Saved' in df.columns:
            chose_majority_rate = df[df['Saved'] == 1]['is_majority'].mean()
            f.write(f"- **å¤šæ•¸é¸æ“‡ç‡**: {chose_majority_rate*100:.1f}%\n")
        
        # è¡çªå ´æ™¯æ¯”ä¾‹
        if 'lawful_vs_majority_conflict' in df.columns:
            conflict_rate = df.groupby('ResponseID')['lawful_vs_majority_conflict'].first().mean()
            f.write(f"- **è¡çªå ´æ™¯æ¯”ä¾‹**: {conflict_rate*100:.1f}%\n\n")
        
        # åœ‹å®¶å±¤ç´šç‰¹å¾µ
        country_features = [col for col in df.columns if col.startswith('country_')]
        if country_features:
            f.write("### åœ‹å®¶å±¤ç´šç‰¹å¾µ\n\n")
            f.write("å¾ `CountriesChangePr.csv` æ•´åˆåœ‹å®¶çš„é“å¾·åå¥½ AMCE å€¼ï¼Œç”¨æ–¼éšå±¤ç·šæ€§æ¨¡å‹åˆ†æã€‚\n\n")
            f.write("**å·²æ•´åˆçš„åœ‹å®¶ç‰¹å¾µ**:\n\n")
            
            for feat in country_features:
                desc = descriptions.get(feat, '')
                if feat in df.columns:
                    mean_val = df[feat].mean()
                    std_val = df[feat].std()
                    f.write(f"- `{feat}`: {desc}\n")
                    f.write(f"  - å¹³å‡å€¼: {mean_val:.3f}, æ¨™æº–å·®: {std_val:.3f}\n")
            f.write("\n")
        
        f.write("### ä½¿ç”¨è€…é“å¾·å´å¯«\n\n")
        f.write("åŸºæ–¼ä½¿ç”¨è€…åœ¨è¡çªå ´æ™¯ä¸­çš„é¸æ“‡ï¼Œå»ºç«‹é“å¾·å‚¾å‘å´å¯«ã€‚\n\n")
        f.write("**âš ï¸ é‡è¦ï¼šç‚ºé¿å…è³‡æ–™æ´©æ¼ï¼Œä½¿ç”¨è€…å´å¯«åˆ†åˆ¥åŸºæ–¼è¨“ç·´é›†å’Œæ¸¬è©¦é›†è¨ˆç®—**\n\n")
        f.write("- è¨“ç·´é›†ä½¿ç”¨è€…å´å¯«ï¼šåƒ…ä½¿ç”¨è¨“ç·´é›†è³‡æ–™è¨ˆç®—\n")
        f.write("- æ¸¬è©¦é›†ä½¿ç”¨è€…å´å¯«ï¼šåƒ…ä½¿ç”¨æ¸¬è©¦é›†è³‡æ–™è¨ˆç®—\n")
        f.write("- ä½¿ç”¨è€…å´å¯«**ä¸æ‡‰ç”¨æ–¼é æ¸¬æ¨¡å‹**ï¼Œåƒ…ç”¨æ–¼æ¢ç´¢æ€§åˆ†æ\n\n")
        
        f.write("| å´å¯«æŒ‡æ¨™ | èªªæ˜ | å¹³å‡å€¼ | æ¨™æº–å·® |\n")
        f.write("|---------|------|--------|--------|\n")
        
        profile_features = ['utilitarian_score', 'deontology_score', 
                           'consistency_score', 'n_scenarios']
        
        for feat in profile_features:
            if feat in profiles_df.columns:
                desc = descriptions.get(feat, '')
                mean_val = profiles_df[feat].mean()
                std_val = profiles_df[feat].std()
                f.write(f"| {feat} | {desc} | {mean_val:.3f} | {std_val:.3f} |\n")
        
        f.write(f"\n- **å´å¯«ä½¿ç”¨è€…æ•¸**: {len(profiles_df):,} ä½\n")
        f.write(f"- **å¹³å‡å®Œæˆå ´æ™¯æ•¸**: {profiles_df['n_scenarios'].mean():.1f} å€‹\n\n")
        
        f.write("### é“å¾·å‚¾å‘åˆ†ä½ˆ\n\n")
        
        if 'utilitarian_score' in profiles_df.columns:
            # åˆ†é¡ä½¿ç”¨è€…
            strong_util = (profiles_df['utilitarian_score'] > 0.7).sum()
            moderate_util = ((profiles_df['utilitarian_score'] >= 0.3) & 
                           (profiles_df['utilitarian_score'] <= 0.7)).sum()
            weak_util = (profiles_df['utilitarian_score'] < 0.3).sum()
            
            f.write("**æ•ˆç›Šä¸»ç¾©å‚¾å‘åˆ†ä½ˆ**:\n\n")
            f.write(f"- å¼·æ•ˆç›Šä¸»ç¾© (>0.7): {strong_util:,} ä½ ({strong_util/len(profiles_df)*100:.1f}%)\n")
            f.write(f"- ä¸­é–“æ´¾ (0.3-0.7): {moderate_util:,} ä½ ({moderate_util/len(profiles_df)*100:.1f}%)\n")
            f.write(f"- å¼·ç¾©å‹™è«– (<0.3): {weak_util:,} ä½ ({weak_util/len(profiles_df)*100:.1f}%)\n\n")
        
        f.write("### è¨“ç·´/æ¸¬è©¦é›†åˆ†å‰²\n\n")
        f.write("æ¡ç”¨ä½¿ç”¨è€…å±¤ç´šåˆ†å‰²ï¼ˆ80/20ï¼‰ï¼Œç¢ºä¿åŒä¸€ä½¿ç”¨è€…çš„è³‡æ–™ä¸æœƒåŒæ™‚å‡ºç¾åœ¨è¨“ç·´é›†å’Œæ¸¬è©¦é›†ï¼Œé¿å…è³‡æ–™æ´©æ¼ã€‚\n\n")
        
        train_file = Path('data/processed/train_data.csv')
        test_file = Path('data/processed/test_data.csv')
        
        if train_file.exists() and test_file.exists():
            train_df_check = pd.read_csv(train_file, nrows=1000)
            test_df_check = pd.read_csv(test_file, nrows=1000)
            
            f.write(f"- **è¨“ç·´é›†**: ç´„ {len(df)*0.8:,.0f} è¡Œ\n")
            f.write(f"- **æ¸¬è©¦é›†**: ç´„ {len(df)*0.2:,.0f} è¡Œ\n\n")
        
        f.write("### ç‰¹å¾µä½¿ç”¨æ³¨æ„äº‹é …\n\n")
        f.write("ä¸åŒåˆ†æéšæ®µé©ç”¨çš„ç‰¹å¾µï¼š\n\n")
        f.write("**ç¬¬3ç«  æ¢ç´¢æ€§åˆ†æ**:\n")
        f.write("- âœ… å ´æ™¯å±¤ç´šç‰¹å¾µ\n")
        f.write("- âœ… åœ‹å®¶å±¤ç´šç‰¹å¾µ\n")
        f.write("- âœ… ä½¿ç”¨è€…é“å¾·å´å¯«ï¼ˆç”¨æ–¼åˆ†ç¾¤å’Œæè¿°ï¼‰\n\n")
        f.write("**ç¬¬4ç«  çµ±è¨ˆæ¨è«–**:\n")
        f.write("- âœ… å ´æ™¯å±¤ç´šç‰¹å¾µ\n")
        f.write("- âœ… åœ‹å®¶å±¤ç´šç‰¹å¾µï¼ˆç”¨æ–¼éšå±¤ç·šæ€§æ¨¡å‹ï¼‰\n")
        f.write("- âŒ ä½¿ç”¨è€…é“å¾·å´å¯«ï¼ˆé¿å…å¾ªç’°è«–è­‰ï¼‰\n\n")
        f.write("**ç¬¬5ç«  é æ¸¬æ¨¡å‹**:\n")
        f.write("- âœ… å ´æ™¯å±¤ç´šç‰¹å¾µ\n")
        f.write("- âœ… äººå£çµ±è¨ˆè®Šæ•¸\n")
        f.write("- âœ… æ–‡åŒ–åœˆåˆ†é¡\n")
        f.write("- âŒ ä½¿ç”¨è€…é“å¾·å´å¯«ï¼ˆæœƒé€ æˆè³‡æ–™æ´©æ¼ï¼‰\n\n")
        
        f.write("ç‰¹å¾µå·¥ç¨‹å®Œæˆå¾Œï¼Œè³‡æ–™å·²æº–å‚™å¥½é€²è¡Œæ¢ç´¢æ€§åˆ†æå’Œå»ºæ¨¡ã€‚\n\n")
    
    print(f"âœ… Markdownå ±å‘Š: {report_file}")

def main():
    """ä¸»åŸ·è¡Œå‡½æ•¸"""
    print("\n" + "=" * 60)
    print("ğŸ”§ MIT Moral Machine - ç‰¹å¾µå·¥ç¨‹ (Step 03)")
    print("=" * 60)
    
    # è¨­å®šæª”æ¡ˆæ—¥èªŒ
    logger = setup_file_logger()
    logger.info("é–‹å§‹åŸ·è¡Œç‰¹å¾µå·¥ç¨‹è…³æœ¬...")
    
    try:
        # Step 1: è¼‰å…¥è³‡æ–™
        print("\nã€Step 1ã€‘è¼‰å…¥è³‡æ–™...")
        
        # è¼‰å…¥æ¸…ç†å¾Œçš„è³‡æ–™
        cleaned_file = Path('data/processed/cleaned_survey.csv')
        if not cleaned_file.exists():
            print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°æª”æ¡ˆ {cleaned_file}")
            print("è«‹å…ˆåŸ·è¡Œ 02_data_cleaning.py")
            return
        
        df = pd.read_csv(cleaned_file)
        print(f"âœ… è¼‰å…¥æ¸…ç†å¾Œè³‡æ–™: {len(df):,} è¡Œ")
        
        # è¼‰å…¥åœ‹å®¶è®ŠåŒ–æ¦‚ç‡è³‡æ–™
        countries_change_file = Path('data/raw/CountriesChangePr.csv')
        if not countries_change_file.exists():
            print(f"\nâš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° {countries_change_file}")
            print("å°‡è·³éåœ‹å®¶å±¤ç´šç‰¹å¾µåˆä½µ")
            countries_change_df = None
        else:
            countries_change_df = pd.read_csv(countries_change_file)
            print(f"âœ… è¼‰å…¥åœ‹å®¶å±¤ç´šè³‡æ–™: {len(countries_change_df)} å€‹åœ‹å®¶")
        
        # Step 2: å»ºç«‹å ´æ™¯ç‰¹å¾µ
        print("\nã€Step 2ã€‘å»ºç«‹å ´æ™¯ç‰¹å¾µ...")
        engineer = FeatureEngineer()
        df_featured = engineer.engineer_features(df)
        
        # Step 3: åˆä½µåœ‹å®¶å±¤ç´šç‰¹å¾µ
        if countries_change_df is not None:
            print("\nã€Step 3ã€‘åˆä½µåœ‹å®¶å±¤ç´šç‰¹å¾µ...")
            df_featured = engineer.merge_country_features(df_featured, countries_change_df)
            
            # æ–°å¢ï¼šå¢åŠ ç‰¹å¾µå¯ç”¨æ€§æ¨™è¨˜
            df_featured = engineer.add_feature_availability_flag(df_featured)
        else:
            print("\nã€Step 3ã€‘è·³éåœ‹å®¶å±¤ç´šç‰¹å¾µåˆä½µ")
        
        # Step 4: åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†ï¼ˆåœ¨è¨ˆç®—ä½¿ç”¨è€…å´å¯«ä¹‹å‰ï¼ï¼‰
        print("\nã€Step 4ã€‘åˆ†å‰²è¨“ç·´/æ¸¬è©¦é›†...")
        train_df, test_df = engineer.split_train_test(df_featured)
        
        # Step 5: åˆ†åˆ¥è¨ˆç®—è¨“ç·´é›†å’Œæ¸¬è©¦é›†çš„ä½¿ç”¨è€…å´å¯«ï¼ˆé¿å…è³‡æ–™æ´©æ¼ï¼‰
        print("\nã€Step 5ã€‘å»ºç«‹ä½¿ç”¨è€…é“å¾·å´å¯«...")
        print("\n  è¨ˆç®—è¨“ç·´é›†ä½¿ç”¨è€…å´å¯«...")
        train_user_profiles = engineer.create_user_profiles(train_df)
        train_user_profiles['split'] = 'train'  # æ¨™è¨˜ç‚ºè¨“ç·´é›†
        
        print("\n  è¨ˆç®—æ¸¬è©¦é›†ä½¿ç”¨è€…å´å¯«...")
        test_user_profiles = engineer.create_user_profiles(test_df)
        test_user_profiles['split'] = 'test'  # æ¨™è¨˜ç‚ºæ¸¬è©¦é›†
        
        # åˆä½µå´å¯«ï¼ˆåƒ…ç”¨æ–¼å„²å­˜å’Œå ±å‘Šï¼Œä¸ç”¨æ–¼é æ¸¬ï¼‰
        all_user_profiles = pd.concat([train_user_profiles, test_user_profiles], ignore_index=True)
        
        print(f"\n  ç¸½è¨ˆ: {len(all_user_profiles):,} ä½ä½¿ç”¨è€…")
        print(f"    è¨“ç·´é›†: {len(train_user_profiles):,} ä½")
        print(f"    æ¸¬è©¦é›†: {len(test_user_profiles):,} ä½")
        
        # Step 6: å„²å­˜çµæœ
        print("\nã€Step 6ã€‘å„²å­˜çµæœ...")
        save_featured_data(df_featured)
        save_user_profiles(all_user_profiles)
        save_train_test_split(train_df, test_df)
        
        # Step 7: ç”¢ç”Ÿå ±å‘Š
        print("\nã€Step 7ã€‘ç”¢ç”Ÿå ±å‘Š...")
        feature_descriptions = engineer.get_feature_descriptions()
        save_feature_descriptions(feature_descriptions)
        generate_feature_statistics(df_featured)
        generate_markdown_report(df_featured, all_user_profiles, feature_descriptions)
        
        # å®Œæˆ
        print("\n" + "=" * 60)
        print("âœ… ç‰¹å¾µå·¥ç¨‹å®Œæˆï¼")
        print("=" * 60)
        print("\nğŸ“Š å·²ç”¢ç”Ÿä»¥ä¸‹è¼¸å‡º:")
        print("  - data/processed/featured_data.csv")
        print("  - data/processed/user_moral_profiles.csv")
        print("  - data/processed/train_data.csv")
        print("  - data/processed/test_data.csv")
        print("  - data/processed/train_test_split.json")
        print("  - outputs/logs/feature_engineering.log")
        print("  - outputs/tables/chapter2/feature_descriptions.csv")
        print("  - outputs/tables/chapter2/scenario_feature_stats.csv")
        print("  - report/drafts/chapter2_section3_feature_engineering.md")
        print("\nâš ï¸  é‡è¦æé†’ï¼š")
        print("  - ä½¿ç”¨è€…å´å¯«å·²åˆ†åˆ¥åŸºæ–¼è¨“ç·´é›†å’Œæ¸¬è©¦é›†è¨ˆç®—")
        print("  - é æ¸¬æ¨¡å‹æ™‚è«‹å‹¿ä½¿ç”¨ä½¿ç”¨è€…å´å¯«ç‰¹å¾µï¼ˆé¿å…è³‡æ–™æ´©æ¼ï¼‰")
        print("  - ä½¿ç”¨è€…å´å¯«åƒ…ç”¨æ–¼ç¬¬3ç« æ¢ç´¢æ€§åˆ†æ")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥: python scripts/04_descriptive_analysis.py")
        print("=" * 60 + "\n")
        
        logger.info("ç‰¹å¾µå·¥ç¨‹è…³æœ¬åŸ·è¡Œå®Œæˆ")
        
    except Exception as e:
        logger.error(f"åŸ·è¡Œå¤±æ•—: {e}", exc_info=True)
        print(f"\nâŒ éŒ¯èª¤: {e}")
        raise

if __name__ == '__main__':
    main()